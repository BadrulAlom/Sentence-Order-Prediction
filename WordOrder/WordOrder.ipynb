{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!---\n",
    "Latex Macros\n",
    "-->\n",
    "$$\n",
    "\\newcommand{\\bar}{\\,|\\,}\n",
    "\\newcommand{\\Xs}{\\mathcal{X}}\n",
    "\\newcommand{\\Ys}{\\mathcal{Y}}\n",
    "\\newcommand{\\y}{\\mathbf{y}}\n",
    "\\newcommand{\\weights}{\\mathbf{w}}\n",
    "\\newcommand{\\balpha}{\\boldsymbol{\\alpha}}\n",
    "\\newcommand{\\bbeta}{\\boldsymbol{\\beta}}\n",
    "\\newcommand{\\aligns}{\\mathbf{a}}\n",
    "\\newcommand{\\align}{a}\n",
    "\\newcommand{\\source}{\\mathbf{s}}\n",
    "\\newcommand{\\target}{\\mathbf{t}}\n",
    "\\newcommand{\\ssource}{s}\n",
    "\\newcommand{\\starget}{t}\n",
    "\\newcommand{\\repr}{\\mathbf{f}}\n",
    "\\newcommand{\\repry}{\\mathbf{g}}\n",
    "\\newcommand{\\x}{\\mathbf{x}}\n",
    "\\newcommand{\\prob}{p}\n",
    "\\newcommand{\\vocab}{V}\n",
    "\\newcommand{\\params}{\\boldsymbol{\\theta}}\n",
    "\\newcommand{\\param}{\\theta}\n",
    "\\DeclareMathOperator{\\perplexity}{PP}\n",
    "\\DeclareMathOperator{\\argmax}{argmax}\n",
    "\\DeclareMathOperator{\\argmin}{argmin}\n",
    "\\newcommand{\\train}{\\mathcal{D}}\n",
    "\\newcommand{\\counts}[2]{\\#_{#1}(#2) }\n",
    "\\newcommand{\\length}[1]{\\text{length}(#1) }\n",
    "\\newcommand{\\indi}{\\mathbb{I}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In the last assignment, you will apply deep learning methods to solve a particular story understanding problem. Automatic understanding of stories is an important task in natural language understanding [[1]](http://anthology.aclweb.org/D/D13/D13-1020.pdf). Specifically, you will develop a model that given a sequence of sentences learns to sort these sentence in order to yield a coherent story [[2]](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/short-commonsense-stories.pdf). This sounds (and to an extent is) trivial for humans, however it is a quite difficult task for machines as it involves commonsense knowledge and temporal understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "You are given a dataset of 45502 instances, each consisting of 5 sentences. Your system needs to ouput a sequence of numbers which represent the predicted order of these sentences. For example, given a story:\n",
    "\n",
    "    He went to the store.\n",
    "    He found a lamp he liked.\n",
    "    He bought the lamp.\n",
    "    Jan decided to get a new lamp.\n",
    "    Jan's lamp broke.\n",
    "\n",
    "your system needs to provide an answer in the following form:\n",
    "\n",
    "    2\t3\t4\t1\t0\n",
    "\n",
    "where the numbers correspond to the zero-based index of each sentence in the correctly ordered story. So \"`2`\" for \"`He went to the store.`\" means that this sentence should come 3rd in the correctly ordered target story. In This particular example, this order of indices corresponds to the following target story:\n",
    "\n",
    "    Jan's lamp broke.\n",
    "    Jan decided to get a new lamp.\n",
    "    He went to the store.\n",
    "    He found a lamp he liked.\n",
    "    He bought the lamp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "To develop your model(s), we provide a training and a development datasets. The test dataset will be held out, and we will use it to evaluate your models. The test set is coming from the same task distribution, and you don't need to expect drastic changes in it.\n",
    "\n",
    "You will use [TensorFlow](https://www.tensorflow.org/) to build a deep learning model for the task. We provide a very crude system which solves the task with a low accuracy, and a set of additional functions you will have to use to save and load the model you create so that we can run it.\n",
    "\n",
    "As we have to run the notebooks of each submission, and as deep learning models take long time to train, your notebook **NEEDS** to conform to the following requirements:\n",
    "* You **NEED** to run your parameter optimisation offline, and provide your final model saved by using the provided function\n",
    "* The maximum size of a zip file you can upload to moodle is 160MB. We will **NOT** allow submissions larger than that.\n",
    "* We do not have time to train your models from scratch! You **NEED** to provide the full code you used for the training of your model, but by all means you **CANNOT** call the training method in the notebook you will send to us.\n",
    "* We will run these notebooks automatically. If your notebook runs the training procedure, in addition to loading the model, and we need to edit your code to stop the training, you will be penalised with **-20 points**.\n",
    "* If you do not provide a pretrained model, and rely on training your model on our machines, you will get **0 points**.\n",
    "* It needs to be tested on the stat-nlp-book Docker setup to ensure that it does not have any dependencies outside of those that we provide. If your submission fails to adhere to this requirement, you will get **0 points**.\n",
    "\n",
    "Running time and memory issues:\n",
    "* We have tested a possible solution on a mid-2014 MacBook Pro, and a few epochs of the model run in less than 3min. Thus it is possible to train a model on the data in reasonable time. However, be aware that you will need to run these models many times over, for a larger number of epochs (more elaborate models, trained on much larger datasets can train for weeks! However, this shouldn't be the case here.). If you find training times too long for your development cycle you can reduce the training set size. Once you have found a good solution you can increase the size again. Caveat: model parameters tuned on a smaller dataset may not be optimal for a larger training set.\n",
    "* In addition to this, as your submission is capped by size, feel free to experiment with different model sizes, numeric values of different precisions, filtering the vocabulary size, downscaling some vectors, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hints\n",
    "\n",
    "A non-exhaustive list of things you might want to give a try:\n",
    "- better tokenization\n",
    "- experiment with pre-trained word representations such as [word2vec](https://code.google.com/archive/p/word2vec/), or [GloVe](http://nlp.stanford.edu/projects/glove/). Be aware that these representations might take a lot of parameters in your model. Be sure you use only the words you expect in the training/dev set and account for OOV words. When saving the model parameters, pre-rained word embeddings can simply be used in the word embedding matrix of your model. As said, make sure that this word embedding matrix does not contain all of word2vec or GloVe. Your submission is limited, and we will not allow uploading nor using the whole representations set (up to 3GB!)\n",
    "- reduced sizes of word representations\n",
    "- bucketing and batching (our implementation is deliberately not a good one!)\n",
    "  - make sure to draw random batches from the data! (we do not provide this in our code!)\n",
    "- better models:\n",
    "  - stacked RNNs (see tf.nn.rnn_cell.MultiRNNCel\n",
    "  - bi-directional RNNs\n",
    "  - attention\n",
    "  - word-by-word attention\n",
    "  - conditional encoding\n",
    "  - get model inspirations from papers on nlp.stanford.edu/projects/snli/\n",
    "  - sequence-to-sequence encoder-decode architecture for producing the right ordering\n",
    "- better training procedure:\n",
    "  - different training algorithms\n",
    "  - dropout on the input and output embeddings (see tf.nn.dropout)\n",
    "  - L2 regularization (see tf.nn.l2_loss)\n",
    "  - gradient clipping (see tf.clip_by_value or tf.clip_by_norm)\n",
    "- model selection:\n",
    "  - early stopping\n",
    "- hyper-parameter optimization (e.g. random search or grid search (expensive!))\n",
    "    - initial learning rate\n",
    "    - dropout probability\n",
    "    - input and output size\n",
    "    - L2 regularization\n",
    "    - gradient clipping value\n",
    "    - batch size\n",
    "    - ...\n",
    "- post-processing\n",
    "  - for incorporating consistency constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Instructions\n",
    "It is important that this file is placed in the **correct directory**. It will not run otherwise. The correct directory is\n",
    "\n",
    "    DIRECTORY_OF_YOUR_BOOK/assignments/2016/assignment3/problem/group_X/\n",
    "    \n",
    "where `DIRECTORY_OF_YOUR_BOOK` is a placeholder for the directory you downloaded the book to, and in `X` in `group_X` contains the number of your group.\n",
    "\n",
    "After you placed it there, **rename the notebook file** to `group_X`.\n",
    "\n",
    "The notebook is pre-set to save models in\n",
    "\n",
    "    DIRECTORY_OF_YOUR_BOOK/assignments/2016/assignment3/problem/group_X/model/\n",
    "\n",
    "Be sure not to tinker with that - we expect your submission to contain a `model` subdirectory with a single saved model! \n",
    "The saving procedure might overwrite the latest save, or not. Make sure you understand what it does, and upload only a single model! (for more details check tf.train.Saver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Instructions\n",
    "This notebook will be used by you to provide your solution, and by us to both assess your solution and enter your marks. It contains three types of sections:\n",
    "\n",
    "1. **Setup** Sections: these sections set up code and resources for assessment. **Do not edit, move nor copy these cells**.\n",
    "2. **Assessment** Sections: these sections are used for both evaluating the output of your code, and for markers to enter their marks. **Do not edit, move, nor copy these cells**.\n",
    "3. **Task** Sections: these sections require your solutions. They may contain stub code, and you are expected to edit this code. For free text answers simply edit the markdown field.  \n",
    "\n",
    "**If you edit, move or copy any of the setup, assessments and mark cells, you will be penalised with -20 points**.\n",
    "\n",
    "Note that you are free to **create additional notebook cells** within a task section. \n",
    "\n",
    "Please **do not share** this assignment nor the dataset publicly, by uploading it online, emailing it to friends etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Instructions\n",
    "\n",
    "To submit your solution:\n",
    "\n",
    "* Make sure that your solution is fully contained in this notebook. Make sure you do not use any additional files other than your saved model.\n",
    "* Make sure that your solution runs linearly from start to end (no execution hops). We will run your notebook in that order.\n",
    "* **Before you submit, make sure your submission is tested on the stat-nlp-book Docker setup to ensure that it does not have any dependencies outside of those that we provide. If your submission fails to adhere to this requirement, you will get 0 points**.\n",
    "* **If running your notebook produces a trivially fixable error that we spot, we will correct it and penalise you with -20 points. Otherwise you will get 0 points for that solution.**\n",
    "* **Rename this notebook to your `group_X`** (where `X` is the number of your group), and adhere to the directory structure requirements, if you have not already done so. ** Failure to do so will result in -1 point.**\n",
    "* Download the notebook in Jupyter via *File -> Download as -> Notebook (.ipynb)*.\n",
    "* Your submission should be a zip file containing the `group_X` directory, containing `group_X.ipynb` notebook, and the `model` directory with _____\n",
    "* Upload that file to the Moodle submission site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='green'>Setup 1</font>: Load Libraries\n",
    "This cell loads libraries important for evaluation and assessment of your model. **Do not change, move or copy it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:04:56.249298",
     "start_time": "2016-12-20T12:04:54.376398"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "#! SETUP 1 - DO NOT CHANGE, MOVE NOR COPY\n",
    "import sys, os\n",
    "_snlp_book_dir = \"../../../../../\"\n",
    "sys.path.append(_snlp_book_dir)\n",
    "# docker image contains tensorflow 0.10.0rc0. We will support execution of only that version!\n",
    "import statnlpbook.nn as nn\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='green'>Setup 2</font>: Load Training Data\n",
    "\n",
    "This cell loads the training data. **Do not edit the next cell, nor copy/duplicate it**. Instead refer to the variables in your own code, and slice and dice them as you see fit (but do not change their values). \n",
    "For example, no one stops you from introducing, in the corresponding task section, `my_train` and `my_dev` variables that split the data into different folds.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:04:57.110195",
     "start_time": "2016-12-20T12:04:56.251082"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "#! SETUP 2 - DO NOT CHANGE, MOVE NOR COPY\n",
    "data_path = _snlp_book_dir + \"data/nn/\"\n",
    "data_train = nn.load_corpus(data_path + \"train.tsv\")\n",
    "data_dev = nn.load_corpus(data_path + \"dev.tsv\")\n",
    "assert(len(data_train) == 45502)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Structures\n",
    "\n",
    "Notice that the data is loaded from tab-separated files. The files are easy to read, and we provide the loading functions that load it into a simple data structure. Feel free to check details of the loading.\n",
    "\n",
    "The data structure at hand is an array of dictionaries, each containing a `story` and the `order` entry. `story` is a list of strings, and `order` is a list of integer indices:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Task 1</font>: Model implementation\n",
    "\n",
    "Your primary task in this assignment is to implement a model that produces the right order of the sentences in the dataset.\n",
    "\n",
    "### Preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_MODEL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from random import shuffle\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "import datetime\n",
    "import uuid\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# improved tokenizer\n",
    "token = re.compile(\"[\\w-]+|'m|'t|'ll|'ve|'d|'s|\\'\")\n",
    "def tokenize(input):\n",
    "    return [word.lower() for word in token.findall(input)]\n",
    "\n",
    "# preprocessing pipeline, used to load the data intro a structure required by the model\n",
    "def pipeline(data, vocab=None, max_sent_len_=None, shuffle_sentences=False):\n",
    "    '''\n",
    "    Maps words in training set to embeddings ids. Same as default pipeline function but with \n",
    "    improved tokenization. \n",
    "    \n",
    "    shuffle_sentences: shuffles input sentence order to generate pseudo-examples for training\n",
    "    '''\n",
    "    is_ext_vocab = True\n",
    "    if vocab is None:\n",
    "        is_ext_vocab = False\n",
    "        vocab = {'<PAD>': 0, '<OOV>': 1}\n",
    "\n",
    "    max_sent_len = -1\n",
    "    data_sentences = []\n",
    "    data_orders = []\n",
    "    for instance in data:\n",
    "        sents = []\n",
    "        for sentence in instance['story']:\n",
    "            sent = []\n",
    "            tokenized = tokenize(sentence)\n",
    "            for token in tokenized:\n",
    "                if not is_ext_vocab and token not in vocab:\n",
    "                    vocab[token] = len(vocab)\n",
    "                if token not in vocab:\n",
    "                    token_id = vocab['<OOV>']\n",
    "                else:\n",
    "                    token_id = vocab[token]\n",
    "                sent.append(token_id)\n",
    "            if len(sent) > max_sent_len:\n",
    "                max_sent_len = len(sent)\n",
    "            sents.append(sent)\n",
    "        # allow random permutations of story order\n",
    "        if shuffle_sentences:\n",
    "            zipped = list(zip(sents, instance['order']))\n",
    "            random.shuffle(zipped)\n",
    "            a,b = zip(*zipped)\n",
    "            data_sentences.append(a)\n",
    "            data_orders.append(b)\n",
    "        else:\n",
    "            data_sentences.append(sents)\n",
    "            data_orders.append(instance['order'])\n",
    "\n",
    "    if max_sent_len_ is not None:\n",
    "        max_sent_len = max_sent_len_\n",
    "    out_sentences = np.full([len(data_sentences), 5, max_sent_len], vocab['<PAD>'], dtype=np.int32)\n",
    "\n",
    "    for i, elem in enumerate(data_sentences):\n",
    "        for j, sent in enumerate(elem):\n",
    "            out_sentences[i, j, 0:len(sent)] = sent\n",
    "\n",
    "    out_orders = np.array(data_orders, dtype=np.int32)\n",
    "\n",
    "    return out_sentences, out_orders, vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### MODEL CONFIG ###\n",
    "\n",
    "num_units = 150        # number of units in each LSTMCell\n",
    "num_layers = 5         # number of stacked LSTMs\n",
    "CELL_TYPE = \"GRUCell\"  \n",
    "ACTIV_FUNC = tf.nn.relu6  \n",
    "KEEP_PRB = { 0: 0.8, 1: 0.6, 2: 0.4, 3: 0.3, 4: 0.3,    # variable dropout\n",
    "            5: 0.25, 6: 0.2, 7: 0.2, 8: 0.2, 9: 0.15 } \n",
    "BATCH_SIZE = 25        # Number of batches to split training data into\n",
    "EPOCHS = 5             # Number of complete passes through of data\n",
    "target_size = 5  \n",
    "\n",
    "# ADAM optimizer parameters \n",
    "GRADIENT_CLIP = True\n",
    "LEARNING_RATE = 0.0005  \n",
    "BETA1 = 0.9  \n",
    "BETA2 = 0.999  \n",
    "EPSILON = 1e-8  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:05:00.995336",
     "start_time": "2016-12-20T12:04:59.968153"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "### MODEL ###\n",
    "\n",
    "# train model on combined train and validation set\n",
    "data_train = data_train + data_dev\n",
    "\n",
    "# generate training vocabulary\n",
    "train_stories, _, vocab_train = pipeline(data_train)\n",
    "# get max sentence length\n",
    "max_sent_len = train_stories.shape[2]\n",
    "# total story sequence length (includes padding)\n",
    "num_steps = 5 * max_sent_len   \n",
    "\n",
    "# input data + parameter placeholders \n",
    "with tf.name_scope('Init'):\n",
    "    story = tf.placeholder(tf.int64, [None, None, None], \"story\")        # [batch_size x 5 x max_length]\n",
    "    order = tf.placeholder(tf.int64, [None, None], \"order\")              # [batch_size x 5] (target_size)\n",
    "    batch_size = tf.shape(story)[0]  \n",
    "    keep_prob = tf.placeholder(tf.float32)  # dropout probability\n",
    "\n",
    "\n",
    "with tf.name_scope('Emb'):\n",
    "    sentences = [tf.reshape(x, [batch_size, -1]) for x in tf.split(1, 5, story)]  # 5 times [batch_size x max_length]\n",
    "    initializer = tf.random_uniform_initializer(-0.1, 0.1)\n",
    "    \n",
    "    # embeddings matrix - NOTE Model is trained using GloVe however this code is ommited here to facilitate model loading\n",
    "    embeddings = tf.get_variable(\"X\", shape=[len(vocab_train), 200], initializer=initializer, trainable=True, dtype=tf.float64)\n",
    "    \n",
    "    # sentence tensors with word embeddings\n",
    "    sentences_embedded = [tf.nn.embedding_lookup(embeddings, sentence)   # 5 x [batch_size x max_seq_length x input_size]\n",
    "                         for sentence in sentences]\n",
    "\n",
    "with tf.name_scope('Model'):\n",
    "    # concatenate sentences into single tensor\n",
    "    inputs = tf.cast(tf.concat(1, sentences_embedded), tf.float32)  # [batch_size x (5 * max_seq_length) x embedding_size]\n",
    "    if CELL_TYPE == \"GRUCell\":\n",
    "        base_cell = tf.nn.rnn_cell.GRUCell(num_units, input_size=None, activation=ACTIV_FUNC)\n",
    "    elif CELL_TYPE == \"LSTMCell\":\n",
    "        base_cell = tf.nn.rnn_cell.LSTMCell(num_units, state_is_tuple=True, activation=ACTIV_FUNC)\n",
    "    else:\n",
    "        raise \"CELL_TYPE not recognised\"\n",
    "    # dropout\n",
    "    drop_cell = tf.nn.rnn_cell.DropoutWrapper(base_cell, output_keep_prob = keep_prob)\n",
    "    # stacked cells\n",
    "    cell = tf.nn.rnn_cell.MultiRNNCell([drop_cell] * num_layers, state_is_tuple=True)\n",
    "    # recurrent neural network \n",
    "    outputs, state = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)\n",
    "    '''\n",
    "    outputs dimensions: [batch_size x total_sequence_length x num_units]\n",
    "    state dimensions: [batch_size x tuple(cell.state_size)]\n",
    "    '''\n",
    "    # final linear transform\n",
    "    output = tf.reshape(outputs, [-1, num_steps * num_units])\n",
    "    W = tf.get_variable(\"W\", [num_steps * num_units, 5 * target_size], dtype=tf.float32)\n",
    "    b = tf.get_variable(\"b\", [5 * target_size], dtype=tf.float32)\n",
    "    logits_flat = tf.matmul(output, W) + b  # dimensions: [batch_size x (5 * target_size)]\n",
    "    # unflatten logits (need this shape for sparse softmax)\n",
    "    logits = tf.reshape(logits_flat, [-1, 5, target_size])  # [batch_size x 5 x target_size]\n",
    "\n",
    "with tf.name_scope('Loss'):\n",
    "    # cross entropy loss function\n",
    "    loss = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, order))\n",
    "\n",
    "with tf.name_scope('Train'):\n",
    "    if GRADIENT_CLIP:\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE, beta1=BETA1, beta2=BETA2, epsilon=EPSILON)\n",
    "        gvs = optimizer.compute_gradients(loss)\n",
    "        capped_gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\n",
    "        train_step = optimizer.apply_gradients(capped_gvs)\n",
    "    else:\n",
    "        train_step = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE, beta1=BETA1, beta2=BETA2, \n",
    "            epsilon=EPSILON).minimize(loss)\n",
    "\n",
    "        \n",
    "# prediction function\n",
    "unpacked_logits = [tensor for tensor in tf.unpack(logits, axis=1)]\n",
    "softmaxes = [tf.nn.softmax(tensor) for tensor in unpacked_logits]\n",
    "softmaxed_logits = tf.pack(softmaxes, axis=1)\n",
    "predict = tf.arg_max(softmaxed_logits, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training \n",
    "\n",
    "We defined the preprocessing pipeline, set the model up, so we can finally train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:05:54.615600",
     "start_time": "2016-12-20T12:05:01.186008"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "if TRAIN_MODEL:\n",
    "    for epoch in range(EPOCHS):\n",
    "        with tf.Session() as sess:\n",
    "\n",
    "            shuffle(data_train)   # randomly shuffle training set --> natural random batches\n",
    "            train_stories, train_orders, _ = pipeline(data_train, vocab=vocab_train, shuffle_sentences=(epoch > 0))\n",
    "\n",
    "            # chunks: number of batches to cover entire data set\n",
    "            n = train_stories.shape[0]\n",
    "            chunks = n // BATCH_SIZE\n",
    "\n",
    "            print('----- Epoch', epoch, '-----')\n",
    "            for i in range(chunks):\n",
    "                inds = slice(i * BATCH_SIZE, (i + 1) * BATCH_SIZE, 1)\n",
    "                sess.run(train_step, feed_dict={story: train_stories[inds], order: train_orders[inds], keep_prob: KEEP_PRB[epoch]})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Assessment 1</font>: Assess Accuracy (50 pts) \n",
    "\n",
    "We assess how well your model performs on an unseen test set. We will look at the accuracy of the predicted sentence order, on sentence level, and will score them as followis:\n",
    "\n",
    "* 0 - 20 pts: 45% <= accuracy < 50%, linear\n",
    "* 20 - 40 pts: 50% <= accuracy < 55\n",
    "* 40 - 70 pts 55 <= accuracy < Best Result, linear\n",
    "\n",
    "The **linear** mapping maps any accuracy value between the lower and upper bound linearly to a score. For example, if your model's accuracy score is $acc=54.5\\%$, then your score is $20 + 20\\frac{acc-50}{55-50}$.\n",
    "\n",
    "The *Best-Result* accuracy is the maximum of the best accuracy the course organiser achieved, and the submitted accuracies scores.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Change the following lines so that they construct the test set in the same way you constructed the dev set in the code above. We will insert the test set instead of the dev set here. **`test_feed_dict` variable must stay named the same**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:05:54.755730",
     "start_time": "2016-12-20T12:05:54.617471"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# LOAD THE DATA\n",
    "data_test = nn.load_corpus(data_path + \"dev.tsv\")\n",
    "# make sure you process this with the same pipeline as you processed your dev set\n",
    "test_stories, test_orders, _ = pipeline(data_test, vocab=vocab_train, max_sent_len_=max_sent_len)\n",
    "\n",
    "# THIS VARIABLE MUST BE NAMED `test_feed_dict`\n",
    "test_feed_dict = {story: test_stories, order: test_orders, keep_prob: 1.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Sets dev_orders to be test_orders as bug below and no issue with redundant variable\n",
    "\n",
    "dev_orders = test_orders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code loads your model, computes accuracy, and exports the result. **DO NOT** change this code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-12-20T12:05:55.116609",
     "start_time": "2016-12-20T12:05:54.758571"
    },
    "collapsed": false,
    "run_control": {
     "frozen": false,
     "read_only": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.67471940138963127"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#! ASSESSMENT 1 - DO NOT CHANGE, MOVE NOR COPY\n",
    "with tf.Session() as sess:\n",
    "    # LOAD THE MODEL\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, './model/model.checkpoint')\n",
    "    \n",
    "    # RUN TEST SET EVALUATION\n",
    "    dev_predicted = sess.run(predict, feed_dict=test_feed_dict)\n",
    "    dev_accuracy = nn.calculate_accuracy(dev_orders, dev_predicted)\n",
    "\n",
    "dev_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='orange'>Mark</font>:  Your solution to Task 1 is marked with ** __ points**. \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Task 2</font>: Describe your Approach\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "\n",
    "We focused on three aspects in this project: input preprocessing, RNN architecture, and hyperparameter optimization. We outline them in the following.\n",
    "\n",
    "##### Tokenization: \n",
    "We improved the default tokenizer using regex to split words rather than whitespaces and converted to lowercase to match our word embedding reference (GloVe). \n",
    "\n",
    "##### Word Representation: \n",
    "We use pre-trained [GloVe embeddings](http://nlp.stanford.edu/projects/glove/) based on Wikipedia and Gigaword 5. We used an embedding size of 200, as large sizes exceeded file-size limits. We chose to allow these embeddings to be trainable as we found this improved model performance. \n",
    "\n",
    "##### Model Architecture: \n",
    "Our final model is a RNN of stacked GRU units where the input is a concatenation of padded sentences. The RNN is followed by a linear layer and softmax, that we optimize using the cross entropy objective. We found LSTM and GRU units to give similar performance, with GRU converging faster. We also tested various activation functions and found ReLU6 to perform best. We used dropout layers between each GRU layer and we increased the level of dropout with each epoch as we found this helped prevent overfitting. \n",
    "\n",
    "##### Training: \n",
    "We used the Adam optimizer with gradient clipping to avoid exploding gradients. Stochastic gradient descent was achieved by shuffling the dataset at each epoch. We also recognized that the model must be invariant to sentence permutations, and so we shuffle the order of each story after every epoch. The figure below shows the training and dev accuracy for the final model, and we can see that the model is overfitting after about 9000 steps. And so, we retrained this model configuration on the combined training and dev sets, stopping early after 9000 steps.\n",
    "\n",
    "<img src=\"train_model.png\" alt=\"Drawing\" style=\"width: 500px; height: 300px\"/>\n",
    "\n",
    "##### Hyperparameter Optimization: \n",
    "One hypothesis was that a higher number of layers would be required for the model to learn both the concept of a sentence, the meaning within the sentence, and the relative order of the meaning against other sentences. However we found that a large number of layers (up to 15 layers) led to a decrease in performance, with 5 being optimal. Similarly we found 150 to be the optimal number of GRU units (i.e. the dimensionality of hidden gates in each cell).\n",
    "\n",
    "##### Error Analysis:\n",
    "From inspecting samples of predictions, we found that our model was predicting 2’s and 3’s the majority of the time. This is due to the positional nature of the selection. We suspect this is a result of the model being uncertain about the temporal ordering of the sentences, having no strong trigger for the initial or final placement. We also found that there were a large number of sentences being classified into 2 or more places, which is clearly suboptimal. We attempted to correct this by comparing the probability distribution for each position in the story and enforcing that there were no duplicates. However, this approach resulted in a drop in model performance. \n",
    "\n",
    "##### Other Approaches:\n",
    "* We implemented a more sophisticated tokenization scheme where we would only split hyphenated words if they did not exist in GloVe. This approach was not used in the end because tensorflow does not allow the saving of dictionary structures (required to match hyphenated words).\n",
    "\n",
    "* We implemented other RNN architectures, which consisted of 5 sentence encoders and an attention mechanism that allowed communication between the hidden states of a sentence encoder and the final hidden state in the next sentence encoder. Having separate sentence encoders allowed us to avoid the use of padding as the tensorflow RNN implementation allowed variable length sequences. This model was inspired by this [paper](https://arxiv.org/pdf/1509.06664.pdf), and we achieved a 55% accuracy on the dev set (see figure below). <b>Code for this model is provided at the end of the notebook.</b> \n",
    "\n",
    "* We also made use of tensorflow's bidirectional RNN implementation with both model architectures. However, we saw no improvement in performance. \n",
    "\n",
    "<img src=\"seq_model.png\" alt=\"Drawing\" style=\"width: 500px; height: 300px\"/>\n",
    "\n",
    "##### Comparison with Default Model\n",
    "Our story processing is different from the default model because we make use of existing word representations (glove) and we do not simply take the sum of words in each sentence. Instead, we concatenate the 5 sentences into one sequence and use that as input to our RNN. We also improved the training algorithm by introducing random batches, as well as a kind of augmented training set with sentences in each story being shuffled after every epoch. Overall, our model improved dev accuracy by about 17% as compared to the default model. \n",
    "\n",
    "##### References:\n",
    "1. [Rocktäschel, Tim, et al. \"Reasoning about entailment with neural attention.\" arXiv preprint arXiv:1509.06664 (2015).](https://arxiv.org/pdf/1509.06664.pdf)\n",
    "2. [Barzilay, Regina, Noemie Elhadad, and Kathleen R. McKeown. \"Sentence ordering in multidocument summarization.\"](http://www.cs.columbia.edu/nlp/papers/2001/barzilay_al_01.pdf)\n",
    "3. [Triantafillou, Eleni, et al. \"Towards Generalizable Sentence Embeddings.\" ACL 2016 (2016): 239.](http://www.cs.toronto.edu/~zemel/documents/eleniACL.pdf)\n",
    "4. [RNNs in Tensorflow](http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/)\n",
    "5. [Cs224d Tensorflow Tutorial](https://cs224d.stanford.edu/lectures/CS224d-Lecture7.pdf)\n",
    "6. [Lajanugen Logeswaran, Honglak Lee & Dragomir Radev \"SENTENCE ORDERING USING RECURRENT NEURAL NETWORKS\"](https://openreview.net/pdf?id=S1AG8zYeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Assessment 2</font>: Assess Description (30 pts) \n",
    "\n",
    "We will mark the description along the following dimensions: \n",
    "\n",
    "* Clarity (10pts: very clear, 0pts: we can't figure out what you did, or you did nothing)\n",
    "* Creativity (10pts: we could not have come up with this, 0pts: Use only the provided model)\n",
    "* Substance (10pts: implemented complex state-of-the-art classifier, compared it to a simpler model, 0pts: Only use what is already there)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='orange'>Mark</font>:  Your solution to Task 2 is marked with ** __ points**.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='orange'>Final mark</font>: Your solution to Assignment 3 is marked with ** __points**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------\n",
    "# Appendix: Code for our alternative models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Sentence encoder to story encoder model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unused1():\n",
    "    ### MODEL CONFIGURATION ###\n",
    "    BATCH_SIZE = 25                 # batch size\n",
    "    EPOCHS = 5                     # epochs\n",
    "\n",
    "    num_units = 40                    # number of units in each LSTM (sentence encoding)\n",
    "    num_layers = 1                     # number of stacked LSTMs  (sentence encoding)\n",
    "    KEEP_PRB_1 = 0.99                    # dropout (sentence encoding)\n",
    "\n",
    "    _units = 60                         # number of units in each LSTM  (story encoding)\n",
    "    _layers = 1                         # number of stacked LSTMs  (story encoding)\n",
    "    KEEP_PRB_2 = 0.75                    # dropout (story encoding)\n",
    "\n",
    "    learning_rate = 0.001            \n",
    "    target_size = 5                \n",
    "\n",
    "    ### placeholders\n",
    "    seq_story = tf.placeholder(tf.int64, [None, None, None], \"story\")        # [batch_size x 5 x max_seq_length]\n",
    "    seq_order = tf.placeholder(tf.int64, [None, None], \"order\")              # [batch_size x 5]\n",
    "    seq_lens = tf.placeholder(tf.int64, [None, None], \"seq_lens\")     # [batch_size x 5]\n",
    "    batch_size = tf.shape(seq_story)[0]\n",
    "    keep_prob = tf.placeholder(tf.float64)          \n",
    "    keep_prob_2 = tf.placeholder(tf.float64)        \n",
    "\n",
    "    with tf.variable_scope(\"seq\"):\n",
    "        # Word embeddings\n",
    "        sentences = [tf.reshape(x, [batch_size, -1]) for x in tf.split(1, 5, seq_story)]  # 5 times [batch_size x max_sent_len]\n",
    "        initializer = tf.random_uniform_initializer(-0.1, 0.1)\n",
    "        embeddings = tf.get_variable(\"embeddings\", shape=[len(vocab), 100], initializer=embeds, trainable=True)\n",
    "        inputs = [tf.nn.embedding_lookup(embeddings, sentence)   # 5 times [batch_size x max_sent_len x embedding_size]\n",
    "                              for sentence in sentences]\n",
    "\n",
    "\n",
    "    with tf.variable_scope(\"lstms\") as varscope:\n",
    "        # first LSTM  (sentence encoder)\n",
    "        index = 0\n",
    "        lstm1 = tf.nn.rnn_cell.LSTMCell(num_units, state_is_tuple=True, activation=tf.nn.relu6)\n",
    "        lstm1 = tf.nn.rnn_cell.DropoutWrapper(lstm1, output_keep_prob=keep_prob)\n",
    "        lstm1 = tf.nn.rnn_cell.MultiRNNCell([lstm1] * num_layers)\n",
    "        out1, state1 = tf.nn.dynamic_rnn(lstm1, inputs[index], dtype=tf.float64, initial_state=None, sequence_length=seq_lens[:,index])\n",
    "        varscope.reuse_variables()\n",
    "\n",
    "        ### second LSTM  (sentence encoder)\n",
    "        index = 1\n",
    "        lstm2 = tf.nn.rnn_cell.LSTMCell(num_units, state_is_tuple=True, activation=tf.nn.relu6)\n",
    "        lstm2 = tf.nn.rnn_cell.DropoutWrapper(lstm2, output_keep_prob=keep_prob)\n",
    "        lstm2 = tf.nn.rnn_cell.MultiRNNCell([lstm2] * num_layers)\n",
    "        out2, state2 = tf.nn.dynamic_rnn(lstm2, inputs[index], dtype=tf.float64, initial_state=state1, sequence_length=seq_lens[:,index])\n",
    "        varscope.reuse_variables()\n",
    "\n",
    "        ### third LSTM  (sentence encoder)\n",
    "        index = 2\n",
    "        lstm3 = tf.nn.rnn_cell.LSTMCell(num_units, state_is_tuple=True, activation=tf.nn.relu6)\n",
    "        lstm3 = tf.nn.rnn_cell.DropoutWrapper(lstm3, output_keep_prob=keep_prob)\n",
    "        lstm3 = tf.nn.rnn_cell.MultiRNNCell([lstm3] * num_layers)\n",
    "        out3, state3 = tf.nn.dynamic_rnn(lstm3, inputs[index], dtype=tf.float64, initial_state=state2, sequence_length=seq_lens[:,index])\n",
    "        varscope.reuse_variables()\n",
    "\n",
    "        ### fourth LSTM  (sentence encoder)\n",
    "        index = 3\n",
    "        lstm4 = tf.nn.rnn_cell.LSTMCell(num_units, state_is_tuple=True, activation=tf.nn.relu6)\n",
    "        lstm4 = tf.nn.rnn_cell.DropoutWrapper(lstm4, output_keep_prob=keep_prob)\n",
    "        lstm4 = tf.nn.rnn_cell.MultiRNNCell([lstm4] * num_layers)\n",
    "        out4, state4 = tf.nn.dynamic_rnn(lstm4, inputs[index], dtype=tf.float64, initial_state=state3, sequence_length=seq_lens[:,index])\n",
    "        varscope.reuse_variables()\n",
    "\n",
    "        ### last LSTM  (sentence encoder)\n",
    "        index = 4\n",
    "        lstm5 = tf.nn.rnn_cell.LSTMCell(num_units, state_is_tuple=True, activation=tf.nn.relu6)\n",
    "        lstm5 = tf.nn.rnn_cell.DropoutWrapper(lstm5, output_keep_prob=keep_prob)\n",
    "        lstm5 = tf.nn.rnn_cell.MultiRNNCell([lstm5] * num_layers)\n",
    "        out5, state5 = tf.nn.dynamic_rnn(lstm5, inputs[index], dtype=tf.float64, initial_state=state4, sequence_length=seq_lens[:,index])\n",
    "        '''\n",
    "        out dimensions: [batch_size x max_sent_len x num_units]\n",
    "        state dimensions: num_layers times [batch_size x num_units]\n",
    "        '''\n",
    "\n",
    "    ### attention implementation based on https://arxiv.org/pdf/1509.06664.pdf ###\n",
    "    def attention(out1, s2, B, L, k, curr_scope):\n",
    "        with tf.variable_scope(curr_scope):\n",
    "            initializer = tf.random_uniform_initializer(-0.1, 0.1)\n",
    "\n",
    "            W_y = tf.get_variable(\"W_y\", shape=[k, k], initializer=initializer, trainable=True, dtype=tf.float64) # [k x k]\n",
    "            W_h = tf.get_variable(\"W_h\", shape=[k, k], initializer=initializer, trainable=True, dtype=tf.float64) # [k x k]\n",
    "            W_p = tf.get_variable(\"W_p\", shape=[k, k], initializer=initializer, trainable=True, dtype=tf.float64) # [k x k]\n",
    "            W_x = tf.get_variable(\"W_x\", shape=[k, k], initializer=initializer, trainable=True, dtype=tf.float64) # [k x k]\n",
    "\n",
    "            w = tf.get_variable(\"w\", shape=[1,k], initializer=initializer, trainable=True, dtype=tf.float64) # [1 x k]\n",
    "\n",
    "            out1_t = tf.transpose(out1, perm=[2,0,1])  # [k x batch_size x L]\n",
    "            Y = tf.reshape(out1_t, [k, -1])   # [k  x (L*batch_size)]\n",
    "            left = tf.matmul(W_y, Y) # [k x (batch_size*L)]\n",
    "            left = tf.reshape(left, [k, B, L])\n",
    "            left = tf.transpose(left, perm=[1, 0, 2])  # [batch_size x k x L]\n",
    "\n",
    "            hN = s2.h # [batch_size x k]\n",
    "            right = tf.matmul(hN, W_h) # [batch_size x k]\n",
    "            right = tf.expand_dims(right, axis=2) # [batch_size x k x 1]\n",
    "\n",
    "            M = tf.tanh(left + right)  # [batch_size x k x L]\n",
    "\n",
    "            M = tf.transpose(M, perm=[1,0,2])\n",
    "            M = tf.reshape(M, [k, -1])  #[k x (L*batch_size)]\n",
    "            wM = tf.matmul(w, M)  # [1 x (L*batch_size)]\n",
    "            wM = tf.reshape(wM, [1, B, L]) # [1 x batch_size x L]\n",
    "            wM = tf.transpose(wM, perm=[1,0,2])  # [batch_size  x 1 x L]\n",
    "\n",
    "            alpha = tf.nn.softmax(wM, dim=-1)  # [batch_size x 1 x L]\n",
    "\n",
    "            r = tf.batch_matmul(alpha, out1)  # [batch_size x 1 x k]\n",
    "\n",
    "            r = tf.transpose(r, perm=[2,0,1])  # [k x batch_size x 1]\n",
    "            r = tf.reshape(r, [k,-1])  # [k x batch_size]\n",
    "            Wpr = tf.matmul(W_p, r)  # [k x batch_size]\n",
    "            Wpr = tf.transpose(Wpr) # [batch_size x k]\n",
    "\n",
    "            Wxhn = tf.matmul(hN, W_x) # [batch_size x k]\n",
    "\n",
    "            h_final = tf.tanh(Wpr + Wxhn)  # [batch_size x k]\n",
    "\n",
    "            return h_final\n",
    "\n",
    "\n",
    "    ### attention output vectors each of dimension [batch_size x num_units]\n",
    "    h_0 = state1[-1].h\n",
    "    h_1 = attention(out1, state2[-1], batch_size, tf.shape(out1)[1], num_units, \"att1\")\n",
    "    h_2 = attention(out2, state3[-1], batch_size, tf.shape(out2)[1], num_units, \"att2\")\n",
    "    h_3 = attention(out3, state4[-1], batch_size, tf.shape(out3)[1], num_units, \"att3\")\n",
    "    h_4 = attention(out4, state5[-1], batch_size, tf.shape(out4)[1], num_units, \"att4\")\n",
    "\n",
    "    with tf.variable_scope(\"seq\"):\n",
    "        # create final input tensor for bidirectional RNN\n",
    "        new_inputs = [h_0, h_1, h_2, h_3, h_4]  # 5 times [batch_size x num_units]\n",
    "        sl = len(new_inputs)\n",
    "        new_inputs = [tf.expand_dims(x,axis=1) for x in new_inputs]\n",
    "        new_inputs = tf.concat(1, new_inputs)  # [batch_size x 5 x num_units]\n",
    "\n",
    "        ### final LSTM (story encoding)\n",
    "        lstm_cell = tf.nn.rnn_cell.LSTMCell(_units, state_is_tuple=True, activation=tf.nn.relu6)\n",
    "        lstm_cell = tf.nn.rnn_cell.DropoutWrapper(lstm_cell, output_keep_prob=keep_prob_2)\n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell([lstm_cell] * _layers, state_is_tuple=True)\n",
    "        ### final RNN (story encoding)\n",
    "        final_outputs, final_state = tf.nn.dynamic_rnn(cell, new_inputs, dtype=tf.float64) \n",
    "        output = tf.reshape(final_outputs, [-1, 5*_units])  # [batch_size x (5*_units)]\n",
    "\n",
    "\n",
    "    with tf.variable_scope(\"seq\"):\n",
    "        # hidden layer\n",
    "        H1 = tf.nn.relu6(tf.contrib.layers.linear(output, 150))\n",
    "        ### final linear transformation\n",
    "        logits_flat = tf.contrib.layers.linear(H1, 5*target_size)  # [batch_size x 5*target_size]\n",
    "        logits = tf.reshape(logits_flat, [-1, 5, target_size]) # dimensions: [batch_size x 5 x target_size]\n",
    "        ### cross entropy loss function\n",
    "        loss = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, seq_order))\n",
    "        tf.summary.scalar('cross_entropy', loss)\n",
    "\n",
    "\n",
    "    ### optimizer\n",
    "    seq_train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    ### prediction function\n",
    "    seq_unpacked_logits = [tensor for tensor in tf.unpack(logits, axis=1)]\n",
    "    seq_softmaxes = [tf.nn.softmax(tensor) for tensor in seq_unpacked_logits]\n",
    "    seq_softmaxed_logits = tf.pack(seq_softmaxes, axis=1)\n",
    "    seq_predict = tf.arg_max(seq_softmaxed_logits, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Unused bi directional model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unused2():\n",
    "    num_steps = 5 * max_sent_len     # total input sequence length\n",
    "\n",
    "    with tf.name_scope('Init'):\n",
    "        story = tf.placeholder(tf.int32, [None, None, None], \"story\")        # [batch_size x 5 x max_length]\n",
    "        order = tf.placeholder(tf.int32, [None, None], \"order\")              # [batch_size x 5] (target_size)\n",
    "        lengths = tf.cast(tf.placeholder(tf.int32, [None, None], \"lengths\"),tf.int64)\n",
    "        batch_size = tf.shape(story)[0]\n",
    "        keep_prob = tf.placeholder(tf.float32)  # dropout probability placeholder\n",
    "\n",
    "    ### Word embeddings\n",
    "    with tf.name_scope('Emb'):\n",
    "        sentences = [tf.reshape(x, [batch_size, -1]) for x in tf.split(1, 5, story)]  # 5 times [batch_size x max_length]\n",
    "        sent_lengths = [x for x in tf.split(1, 5, lengths)]\n",
    "        if USE_GLOVE:\n",
    "            embeddings = tf.get_variable(\"X\", initializer=train_emb, trainable=TRAINABLE_EMBEDDINGS)\n",
    "            ### embedding_size = 300 # not used\n",
    "        else:\n",
    "            initializer = tf.random_uniform_initializer(-0.1, 0.1)\n",
    "            embeddings = tf.get_variable(\"X\", [vocab_size, embedding_size],\n",
    "                initializer=initializer, dtype=tf.float32, trainable=TRAINABLE_EMBEDDINGS)\n",
    "\n",
    "\n",
    "        sentences_embedded = [tf.nn.embedding_lookup(embeddings, sentence)   # 5 x [batch_size x max_seq_length x input_size]\n",
    "                             for sentence in sentences]\n",
    "        tf.histogram_summary('embeddings', embeddings)\n",
    "\n",
    "    with tf.variable_scope(\"encoder\") as varscope:\n",
    "        outputs = []\n",
    "        states=[]\n",
    "        layers=[]\n",
    "        for sent_no, input_sent in enumerate(sentences_embedded): #[batch_size x max_seq_length x embedding_size]\n",
    "            # combine 5 sentences into one long sequence\n",
    "            #inputs = tf.cast(sentences_embedded, tf.int32)  # [batch_size x (5 * max_seq_length) x embedding_size]\n",
    "\n",
    "            base_cell = tf.nn.rnn_cell.LSTMCell(num_units, use_peepholes=False, state_is_tuple=True, activation=tf.nn.relu6)\n",
    "            drop_cell = tf.nn.rnn_cell.DropoutWrapper(base_cell, output_keep_prob = keep_prob)\n",
    "            #cell = tf.nn.rnn_cell.MultiRNNCell([drop_cell] * 1, state_is_tuple=True)\n",
    "\n",
    "            output, state = tf.nn.bidirectional_dynamic_rnn(cell_fw=drop_cell, cell_bw=drop_cell, inputs=input_sent,\n",
    "                                                            sequence_length=tf.squeeze(sent_lengths[sent_no]),\n",
    "                                                            dtype=tf.float32)  # [batch_size x num_units]\n",
    "            outputs.append(output)\n",
    "            states.append(state)\n",
    "            layers.append(tf.concat(1,[h for _ , h in state])) #TODO think this is wrong\n",
    "            varscope.reuse_variables()  #uses same W for all sentances\n",
    "\n",
    "    with tf.name_scope('Combine'):\n",
    "        order_representation = tf.concat(1, layers)  # [batch_size x [5 x num_units]] #TODO and this\n",
    "        print(order_representation.get_shape())\n",
    "        logits_flat = tf.contrib.layers.linear(order_representation, 5 * target_size)  # [batch_size x 5 x target_size]\n",
    "        logits = tf.reshape(logits_flat, [-1, 5, target_size])  # [batch_size x 5 x target_size\n",
    "\n",
    "    ### Training Loss\n",
    "    with tf.name_scope('Loss'):\n",
    "        loss = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, order))\n",
    "\n",
    "    with tf.name_scope('train_step'):\n",
    "        # optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE, beta1=BETA1, beta2=BETA2,\n",
    "                                            epsilon=EPSILON)\n",
    "        gvs = optimizer.compute_gradients(loss)\n",
    "        capped_gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\n",
    "        train_step = optimizer.apply_gradients(capped_gvs)\n",
    "\n",
    "    with tf.name_scope('predict'):\n",
    "        # prediction function\n",
    "        unpacked_logits = [tensor for tensor in tf.unpack(logits, axis=1)]\n",
    "        softmaxes = [tf.nn.softmax(tensor) for tensor in unpacked_logits]\n",
    "        softmaxed_logits = tf.pack(softmaxes, axis=1)\n",
    "        predict = tf.cast(tf.arg_max(softmaxed_logits, 2),dtype=tf.int32)\n",
    "\n",
    "        ### accuracy\n",
    "        correct = tf.equal(predict, order)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "        tf.scalar_summary('accuracy', accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Unused stacked lstm sentence encoder into bi directional rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unused3():\n",
    "    num_steps = 5 * max_sent_len     # total input sequence length\n",
    "\n",
    "    with tf.name_scope('Init'):\n",
    "        story = tf.placeholder(tf.int32, [None, None, None], \"story\")        # [batch_size x 5 x max_length]\n",
    "        order = tf.placeholder(tf.int32, [None, None], \"order\")              # [batch_size x 5] (target_size)\n",
    "        lengths = tf.cast(tf.placeholder(tf.int32, [None, None], \"lengths\"),tf.int64)\n",
    "        batch_size = tf.shape(story)[0]\n",
    "        seq_len = tf.ones([batch_size,1],dtype=tf.int32)*5\n",
    "        keep_prob = tf.placeholder(tf.float32)  # dropout probability placeholder\n",
    "\n",
    "    ### Word embeddings\n",
    "    with tf.name_scope('Emb'):\n",
    "        sentences = [tf.reshape(x, [batch_size, -1]) for x in tf.split(1, 5, story)]  # 5 times [batch_size x max_length]\n",
    "        sent_lengths = [x for x in tf.split(1, 5, lengths)]\n",
    "        if USE_GLOVE:\n",
    "            embeddings = tf.get_variable(\"X\", initializer=train_emb, trainable=TRAINABLE_EMBEDDINGS)\n",
    "            ### embedding_size = 300 # not used\n",
    "        else:\n",
    "            initializer = tf.random_uniform_initializer(-0.1, 0.1)\n",
    "            embeddings = tf.get_variable(\"X\", [vocab_size, embedding_size],\n",
    "                initializer=initializer, dtype=tf.float32, trainable=TRAINABLE_EMBEDDINGS)\n",
    "\n",
    "\n",
    "        sentences_embedded = [tf.nn.embedding_lookup(embeddings, sentence)   # 5 x [batch_size x max_seq_length x input_size]\n",
    "                             for sentence in sentences]\n",
    "        tf.histogram_summary('embeddings', embeddings)\n",
    "\n",
    "    with tf.variable_scope(\"encoder\") as varscope:\n",
    "        outputs = []\n",
    "        states=[]\n",
    "        layers=[]\n",
    "\n",
    "        for sent_no, input_sent in enumerate(sentences_embedded): #[batch_size x max_seq_length x embedding_size]\n",
    "            # combine 5 sentences into one long sequence\n",
    "            base_cell = tf.nn.rnn_cell.LSTMCell(num_units, use_peepholes=False, state_is_tuple=True, activation=tf.nn.relu6)\n",
    "            drop_cell = tf.nn.rnn_cell.DropoutWrapper(base_cell, output_keep_prob = keep_prob)\n",
    "            #cell = tf.nn.rnn_cell.MultiRNNCell([drop_cell] * 1, state_is_tuple=True)\n",
    "            output, state = tf.nn.dynamic_rnn(drop_cell, inputs=input_sent, sequence_length=tf.squeeze(sent_lengths[sent_no]), dtype=tf.float32)\n",
    "            outputs.append(output)\n",
    "\n",
    "            states.append(state)\n",
    "            varscope.reuse_variables()  #uses same W for all sentances\n",
    "\n",
    "    with tf.name_scope('Combine'):\n",
    "        new_in = tf.concat(1,[i for i in outputs])\n",
    "\n",
    "        output_bi, state_bi = tf.nn.bidirectional_dynamic_rnn(cell_fw=drop_cell, cell_bw=drop_cell, inputs=new_in, sequence_length=seq_len, dtype=tf.float32)  # [batch_size x num_units]\n",
    "\n",
    "        order_representation = tf.concat(1, [h for _, h in state_bi]) # [batch_size x [5 x num_units]] #TODO and this\n",
    "        print(order_representation.get_shape())\n",
    "        logits_flat = tf.contrib.layers.linear(order_representation, 5 * target_size)  # [batch_size x 5 x target_size]\n",
    "        logits = tf.reshape(logits_flat, [-1, 5, target_size])  # [batch_size x 5 x target_size\n",
    "    ### Training Loss\n",
    "    with tf.name_scope('Loss'):\n",
    "        loss = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(logits, order))\n",
    "\n",
    "    with tf.name_scope('train_step'):\n",
    "        # optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=LEARNING_RATE, beta1=BETA1, beta2=BETA2,\n",
    "                                            epsilon=EPSILON)\n",
    "        gvs = optimizer.compute_gradients(loss)\n",
    "        capped_gvs = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gvs]\n",
    "        train_step = optimizer.apply_gradients(capped_gvs)\n",
    "\n",
    "    with tf.name_scope('predict'):\n",
    "        # prediction function\n",
    "        unpacked_logits = [tensor for tensor in tf.unpack(logits, axis=1)]\n",
    "        softmaxes = [tf.nn.softmax(tensor) for tensor in unpacked_logits]\n",
    "        softmaxed_logits = tf.pack(softmaxes, axis=1)\n",
    "        predict = tf.cast(tf.arg_max(softmaxed_logits, 2),dtype=tf.int32)\n",
    "\n",
    "        ### accuracy\n",
    "        correct = tf.equal(predict, order)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "        tf.scalar_summary('accuracy', accuracy)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
